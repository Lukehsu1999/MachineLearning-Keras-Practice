#Import IMDB Dataset

from keras.datasets import imdb
(train_data,train_labels),(test_data,test_labels)=imdb.load_data(num_words=10000)

#function: translate from index to word

word_index=imdb.get_word_index()
reverse_word_index=dict([(value,key) for (key,value) in word_index.items()] )

def translate_review(id):
  decoded_review=' '.join([reverse_word_index.get(i-3,'?') for i in train_data[id]])
  print(decoded_review)

translate_review(0)

#function: vectorize sequences: from list to 2 dimension matrix, which is a one-hot matrix

import numpy as np
def vectorize_sequences(sequences, dimension=10000):
  results=np.zeros((len(sequences),dimension))
  for i, sequence in enumerate(sequences):
    results[i,sequence]=1.
  return results
 
x_train=vectorize_sequences(train_data)
x_test=vectorize_sequences(test_data)
y_train=np.asarray(train_labels).astype('float32')
y_test=np.asarray(test_labels).astype('float32')

#build up model:
#one input layer;
#two middle layer;
#one output layer

from keras import models
from keras import layers

model = models.Sequential(); 
model.add(layers.Dense(16,activation='relu',input_shape=(10000,))) #middle layers
model.add(layers.Dense(16,activation='relu'))
model.add(layers.Dense(1,activation='sigmoid')) #output layer

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

#choose the first 10,000 data as validation material

x_val=x_train[:10000]
partial_x_train=x_train[10000:]

y_val=y_train[:10000]
partial_y_train=y_train[10000:]

#start fit(): train 20 rounds(epoch), batch=512 samples

history=model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))

#draw the loss results

import matplotlib.pyplot as plt
history_dict=history.history
loss_values=history_dict['loss']
val_loss_values=history_dict['val_loss']

epochs=range(1,len(loss_values)+1)

plt.plot(epochs, loss_values,'bo',label='Training loss')
plt.plot(epochs,val_loss_values,'b',label='Validation loss')
plt.title('Training vs Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#plt.clf()

#plot the accuracy

acc=history_dict['accuracy']
val_acc=history_dict['val_accuracy']

plt.plot(epochs, acc,'bo',label='Training acc')
plt.plot(epochs,val_acc,'b',label='Validation acc')
plt.title('Training vs Validation acc')
plt.xlabel('Epochs')
plt.ylabel('acc')
plt.legend()
plt.show()

#Note that after epoch 4, there's overfitting problem
#Last part: use the model to predict movie ratings (1: good; 0: bad)

print(model.predict(x_test))
